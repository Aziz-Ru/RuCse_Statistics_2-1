# Distribution:
_A distribution in statistics is a function that shows the possible values for a variable and how often they occur. It is a mathematical description of the spread of the data._
## Variance:
_Variance is a measure of how spread out a set of data is from the mean_
## Expected value:
_Expected value is the average value of a random variable._ 
## population:
_In statistics, a population is a set of all similar items or events that are of interest for some question or experiment._
## sample:
_In statistics, a sample is a subset of a population._
## parameter:
_In statistics, a parameter is a numerical measure that describes characteristic a population.It is a characteristic of the entire population, not just a sample.Parameters are often denoted by Greek letters, such as μ for the population mean and σ2 for the population variance._
```
    Mean: The average value of all the members of the population.
    Median: The middle value of all the members of the population when arranged in order from least to greatest.
    Mode: The most frequent value of all the members of the population.
    Variance: A measure of how spread out the data is around the mean.
    Standard deviation: The square root of the variance.
```

## statistic:
_A statistic is a numerical measure that describes a sample. It is a characteristic of the sample,  not the population_
## estimator:    
_An estimator is a statistic that estimates some fact about the population._
## Simple random sampling:
_Simple random sampling (SRS) is a probability sampling method in which every member of a population has an equal chance of being selected for the sample._

## Degree of Freedom
**Degrees of freedom (df) is a statistical concept that is used to describe the number of independent values that are used to estimate a parameter.**
## PDF
**The probability density function (pdf) is a function that completely describes the distribution of a continuous random variable.**
## Gamma Funtion
**The Gamma function is a generalization of the factorial function to non-integer numbers.**
## Moment generating function (MGF):
**The moment generating function (MGF) of a random variable is a function that can be used to calculate the moments of the random variable.
The MGF is defined as follows:**
_M(t) = E[e^{tX}]_
## Chi Square Distribution:
**The chi-squared distribution is a continuous probability distribution that is used in a variety of statistical applications, including hypothesis testing, confidence interval estimation, and goodness-of-fit tests**
**The probability density function of the chi-square distribution with n degrees of freedom is given by:**
<img src="https://www.thoughtco.com/thmb/xNWePQ14xqYHLJ_ZV3HjQ_m3G-w=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/ChiSquare-580582515f9b5805c266cc66.jpg">

[chi-square-distribution](https://www.statlect.com/probability-distributions/chi-square-distribution)

[Moment Generating function of of pdf of ch square](https://www.youtube.com/watch?v=7_4joe5OCW0&t=1001s)

[cumulant-generating-function-of-distribution](https://www.assignmenthelp.net/assignment_help/cumulant-generating-function-of-distribution)

## Application:
**1.Hypothesis Testing**

**2.Confidence interval Estimation**

**3.Goodness of fit test**

**4.Testing Independence**

**5.Testing homogenity of varience**
## Properties of the chi-squared distribution:

 **Skewness: The chi-squared distribution is skewed to the right. This means that the right tail of the distribution is longer than the left tail.**

**Mean: The mean of the chi-squared distribution is equal to the degrees of freedom.**

**Variance: The variance of the chi-squared distribution is equal to twice the degrees of freedom.**

**Chi-squared test statistic: The chi-squared test statistic is used to test hypotheses about population parameters. The chi-squared test statistic is a chi-squared variate with degrees of freedom equal to the number of degrees of freedom in the hypothesis test**


## Example od Estimator
_Mean,Median,Mode,Varience,Standard Deviation_
```

There are three main criteria of a good estimator:
Unbiasedness: An unbiased estimator is one whose expected value is equal to the population parameter it is estimating.This means that, on average, the estimator will be equal to the population parameter.

Consistency: A consistent estimator is one that converges to the population parameter as the sample size increases.This means that the estimator will become more and more accurate as the sample size increases.
    
Efficiency: An efficient estimator is one that has the smallest variance among all unbiased estimators of the same population parameter. This means that the estimator will be less variable than other estimators, which will lead to more accurate inferences.


 Sample mean: The sample mean is an unbiased, consistent, and efficient estimator of the population mean.
Sample median: The sample median is a robust estimator of the population median.
Maximum likelihood estimator: The maximum likelihood estimator is a consistent estimator of the population parameter that maximizes the likelihood of the observed data.

Biased: a statistic that is either an overestimate or an underestimate.
Efficient: a statistic with small variances (the one with the smallest possible variance is also called the “best”). Inefficient estimators can give you good results as well, but they usually requires much larger samples.
Invariant: statistics that are not easily changed by transformations, like simple data shifts.
Shrinkage: a raw estimate that’s improved by combining it with other information. See also: The James-Stein estimator.
Sufficient: a statistic that estimates the population parameter as well as if you knew all of the data in all possible samples.
Unbiased: an accurate statistic that neither underestimates nor overestimates.



```
## Types of Estimation:
_Point estimation is the process of estimating a population parameter with a single value. For example, we can use the sample mean to estimate the population mean._

_Interval estimation is the process of estimating a population parameter with a range of values. For example, we can use a confidence interval to estimate the population mean._

_Bayesian estimation is a statistical method for estimating population parameters based on prior knowledge and observed data._

## Likelihood:
The principle of maximum likelihood (MLE) is a statistical method for estimating population parameters. It works by finding the parameter values that maximize the likelihood of the observed data.
The likelihood function is a function of the parameter values and the observed data. It represents the probability of observing the data given the parameter values.The principle of MLE is based on the following idea:
_If we have a set of data and we want to estimate the value of a parameter, then the parameter value that is most likely to have produced the data is the parameter value that maximizes the likelihood of the data._


## sample distribution:
_A sample distribution is the probability distribution of a statistic that is obtained  through repeated sampling of a specific population._
 ##  Normal distribution:    
 _Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data  near the mean are more frequent in occurrence than data far from  the mean._
 
 <img src="https://www.investopedia.com/thmb/lFaG1vgFO0XgA_Xzfw3yPLjG2Iw=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/Clipboard01-fdb217713438416cadafc48a1e4e5ee4.jpg">
 
 ```
 x = value of the variable or data being examined and f(x) the probability function
 μ = the mean
 σ = the standard deviation
A normal distribution becomes a standard normal distribution when it has a mean of 0 and a standard deviation of 1.

```
    

